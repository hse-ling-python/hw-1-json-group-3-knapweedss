{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. –ü–æ–¥—Å—á–µ—Ç —Ç–≤–∏—Ç–æ–≤ –≤ –Ω–∞–±–æ—Ä–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "import pprint\n",
    "twitter = []\n",
    "for line in open('hw_3_twitter.json.txt'):\n",
    "    twitter.append(json.loads(line))\n",
    "twits_number = len(twitter)\n",
    "print('—Ç–≤–∏—Ç–æ–≤ –≤ –Ω–∞–±–æ—Ä–µ 'twits_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. –ü–æ–¥—Å—á–µ—Ç –ø—Ä–æ—Ü–µ–Ω—Ç–∞ —Ç–≤–∏—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–π —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç —É–¥–∞–ª–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü—Ä–æ—Ü–µ–Ω—Ç —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤: 14.162754303599375\n",
      "–ü—Ä–æ—Ü–µ–Ω—Ç —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤ (—Å –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ–º): 14\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(len(twitter)):\n",
    "    if twitter[i].get('delete') is not None:   # –¥–æ—Å—Ç—É–ø –ø–æ –∫–ª—é—á—É delete\n",
    "        a += 1\n",
    "print('–ü—Ä–æ—Ü–µ–Ω—Ç —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤:', (a*100)/twits_number)\n",
    "print('–ü—Ä–æ—Ü–µ–Ω—Ç —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤ (—Å –æ–∫—Ä—É–≥–ª–µ–Ω–∏–µ–º):', (a*100)//twits_number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. –°–∞–º—ã–µ –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —è–∑—ã–∫–∏ —Ç–≤–∏—Ç–æ–≤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('en', 719), ('ja', 438), ('es', 173), ('ko', 149), ('th', 123), ('ar', 119), ('und', 117), ('in', 71), ('pt', 69), ('fr', 35), ('tr', 30), ('tl', 29), ('hi', 23), ('ru', 15), ('fa', 8), ('zh', 8), ('ca', 7)]\n"
     ]
    }
   ],
   "source": [
    "languages = []\n",
    "for i in range(len(twitter)):\n",
    "    if twitter[i].get('lang') is not None:   # –Ω–µ –±–µ—Ä–µ–º deleted twits\n",
    "        languages.append(twitter[i].get('lang'))\n",
    "print(Counter(languages).most_common(17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. –ü–æ–∏—Å–∫ —Ç–≤–∏—Ç–æ–≤ –æ—Ç –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-153-f1508b4887d8>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-153-f1508b4887d8>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    if twitter[i].get('id') —à—ã —Ç—â–µ None:  # —á—Ç–æ–±—ã –Ω–µ —Å—á–∏—Ç–∞–ª–∏—Å—å —É–¥–∞–ª–µ–Ω–Ω—ã–µ —Ç–≤–∏—Ç—ã\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "for i in range(len(twitter)):\n",
    "    if twitter[i].get('id') —à—ã —Ç—â–µ None:  # —á—Ç–æ–±—ã –Ω–µ —Å—á–∏—Ç–∞–ª–∏—Å—å —É–¥–∞–ª–µ–Ω–Ω—ã–µ —Ç–≤–∏—Ç—ã\n",
    "        x.append(twitter[i].get('id'))\n",
    "uniq_users = (len(x) - len(set(x)))  # —Ä–∞–∑–Ω–∏—Ü–∞ –≤—Å–µ—Ö —Ç–≤–∏—Ç–æ–≤ –∏ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö\n",
    "if uniq_users > 0:  # –¥–∞–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ –≤—ã–≤–æ–¥–∞\n",
    "    print('–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ —Ç–≤–∏—Ç–∞–º–∏ ', uniq_users)\n",
    "else:\n",
    "    print('–ù–µ—Ç —Ç–≤–∏—Ç–æ–≤ –æ—Ç –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. –¢–æ–ø-20 —Ö–µ—à—Ç–µ–≥–æ–≤ (–∫–∞–∫ –∂–µ —è –ª—é–±–ª—é –ë–¢–°, –≤–æ—Ç –æ–Ω–∏ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('BTS', 17),\n",
      " ('Î∞©ÌÉÑÏÜåÎÖÑÎã®', 13),\n",
      " ('AMAs', 11),\n",
      " ('‰∫∫Ê∞óÊäïÁ•®„Ç¨„ÉÅ„É£', 8),\n",
      " ('ÌÉúÌòï', 7),\n",
      " ('Î∑î', 6),\n",
      " ('BTSinChicago', 5),\n",
      " ('BTSLoveYourselfTour', 5),\n",
      " ('Ïò§ÎäòÏùòÎ∞©ÌÉÑ', 5),\n",
      " ('PledgeForSwachhBharat', 5),\n",
      " ('MPN', 5),\n",
      " ('PCAs', 4),\n",
      " ('V', 4),\n",
      " ('ÏãúÏπ¥Í≥†1ÌöåÏ∞®Í≥µÏó∞', 4),\n",
      " ('‡πÄ‡∏õ‡πä‡∏Å‡∏ú‡∏•‡∏¥‡∏ï‡πÇ‡∏ä‡∏Ñ', 4),\n",
      " ('JIMIN', 4),\n",
      " ('running', 3),\n",
      " ('NCT', 3),\n",
      " ('ÏßÄÎØº', 3),\n",
      " ('WajahmuPlastik', 3)]\n"
     ]
    }
   ],
   "source": [
    "not_deleted = []  # –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Ç–≤–∏—Ç–æ–≤\n",
    "for i in range(len(twitter)):\n",
    "    if twitter[i].get('entities') is not None:\n",
    "        not_deleted.append(twitter[i])\n",
    "spisok_tegov = []\n",
    "for line in not_deleted:\n",
    "        ent = line['entities']  # –¥–æ—Å—Ç—É–ø –ø–æ entities\n",
    "        hashtags = ent.get('hashtags')  # –¥–æ—Å—Ç—É–ø –ø–æ hashtags\n",
    "        for item in hashtags:\n",
    "            spisok_tegov.append(item.get('text'))  # —Ç–µ–∫—Å—Ç —Å–∞–º–∏—Ö —Ç–µ–≥–æ–≤\n",
    "pprint.pprint(Counter(spisok_tegov).most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ß–∞—Å—Ç–æ—Ç–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å –∏–∑ —Ç–≤–∏—Ç–æ–≤ —Ç–æ–ª—å–∫–æ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º —è–∑—ã–∫–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(' the', 56),\n",
      " (' and', 31),\n",
      " (' that', 19),\n",
      " (' you', 19),\n",
      " (' for', 13),\n",
      " (' your', 13),\n",
      " ('of the', 13),\n",
      " (' with', 11),\n",
      " (' about', 10),\n",
      " (' are', 10),\n",
      " (' from', 10),\n",
      " (' they', 9),\n",
      " (' like', 8),\n",
      " (' but', 8),\n",
      " (' only', 8),\n",
      " (' can', 7),\n",
      " (' &amp', 7),\n",
      " (' out', 7),\n",
      " (' was', 7),\n",
      " (' what', 6),\n",
      " (' get', 6),\n",
      " (' more', 6),\n",
      " (' when', 6),\n",
      " (' than', 6),\n",
      " (' one', 6),\n",
      " (' pardede', 6),\n",
      " (' this', 5),\n",
      " (' people', 5),\n",
      " (' most', 5),\n",
      " ('in the', 5),\n",
      " (' will', 5),\n",
      " (' our', 4),\n",
      " (' know', 4),\n",
      " ('is the', 4),\n",
      " ('o be', 4),\n",
      " (' not', 4),\n",
      " (' while', 4),\n",
      " (' sure', 4),\n",
      " (' better', 4),\n",
      " (' kavanaugh', 4),\n",
      " (' her', 4),\n",
      " (' who', 4),\n",
      " (' time', 4),\n",
      " (' been', 4),\n",
      " (' his', 4),\n",
      " (' run', 4),\n",
      " (' how', 3),\n",
      " (' favorite', 3),\n",
      " (' part', 3),\n",
      " (' back', 3),\n",
      " (' into', 3),\n",
      " (' their', 3),\n",
      " ('on the', 3),\n",
      " (' side', 3),\n",
      " (' ones', 3),\n",
      " (' own', 3),\n",
      " (' week', 3),\n",
      " (' over', 3),\n",
      " (' see', 3),\n",
      " (' all', 3),\n",
      " (' show', 3),\n",
      " (' has', 3),\n",
      " (' just', 3),\n",
      " (' lol', 3),\n",
      " (' lie', 3),\n",
      " (' tju', 3),\n",
      " (' hold', 3),\n",
      " (' kids', 2),\n",
      " (' here', 2),\n",
      " (' top', 2),\n",
      " (' why', 2),\n",
      " (' article', 2),\n",
      " (' night', 2),\n",
      " (' free', 2),\n",
      " (' nsultation', 2),\n",
      " (' check', 2),\n",
      " (' these', 2),\n",
      " (' skin', 2),\n",
      " (' should', 2),\n",
      " (' have', 2),\n",
      " (' good', 2),\n",
      " (\" now'\", 2),\n",
      " (\" 'my\", 2),\n",
      " (' team', 2),\n",
      " ('it is', 2),\n",
      " (' today', 2),\n",
      " (' loving', 2),\n",
      " (' lost', 2),\n",
      " (' link', 2),\n",
      " (' light', 2),\n",
      " (' some', 2),\n",
      " (' stand', 2),\n",
      " (' walk', 2),\n",
      " (' clockwise', 2),\n",
      " (' water', 2),\n",
      " (' bad', 2),\n",
      " (' spilled', 2),\n",
      " (' being', 2),\n",
      " (' stuff', 2),\n",
      " (' i loved', 2),\n",
      " ('it was', 2),\n",
      " (' c at', 2),\n",
      " (' last', 2),\n",
      " (' because', 2),\n",
      " (' point', 2),\n",
      " (' right', 2),\n",
      " (' best', 2),\n",
      " (' way', 2),\n",
      " (' remote', 2),\n",
      " (' below', 2),\n",
      " ('; it', 2),\n",
      " (' helps', 2),\n",
      " (' them', 2),\n",
      " ('o keep', 2),\n",
      " (' insurance', 2),\n",
      " (' creation', 2),\n",
      " (' instruments', 2),\n",
      " (' were', 2),\n",
      " (' during', 2),\n",
      " (' had', 2),\n",
      " (' onethe', 2),\n",
      " (\" it's\", 2),\n",
      " (' rings', 2),\n",
      " (' false', 2),\n",
      " (' series', 2),\n",
      " (' a send', 2),\n",
      " (' cancer', 2),\n",
      " ('is still', 2),\n",
      " (' enough', 2),\n",
      " (\" 'this\", 2),\n",
      " (' project', 2),\n",
      " (' a strong', 2),\n",
      " (' business', 2),\n",
      " (' card', 2),\n",
      " ('in a', 2),\n",
      " (' still', 2),\n",
      " (' join', 2),\n",
      " (' house', 2),\n",
      " (' off', 2),\n",
      " (' i‚Äôve', 2),\n",
      " (' let', 2),\n",
      " (' i‚Äôm', 2),\n",
      " (' after', 2),\n",
      " (' always', 2),\n",
      " (' dance', 2),\n",
      " (' line', 2),\n",
      " ('o make', 2),\n",
      " (' feel', 2),\n",
      " (' osaka', 2),\n",
      " (' make', 2),\n",
      " (' patent', 2),\n",
      " (' legal', 2),\n",
      " (' life', 2),\n",
      " (\" aren't\", 2),\n",
      " (' cat', 2),\n",
      " (' research', 2),\n",
      " ('[\"time', 1),\n",
      " ('o think', 1),\n",
      " (' toys', 1),\n",
      " (\" you'll\", 1),\n",
      " ('be getting', 1),\n",
      " (' christmas', 1),\n",
      " (' picks', 1)]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "en_twits = []\n",
    "a = 'en'  # —Ç–µ–≥ –¥–ª—è –∞–Ω–≥–ª–∏–π—Å–∫–æ–≥–æ\n",
    "for i in range(len(twitter)):\n",
    "    if twitter[i].get('lang') == a:  # –æ—Ç–±–∏—Ä–∞–µ–º —Ç–≤–∏—Ç—ã –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º\n",
    "        if twitter[i].get('extended_tweet') is not None:\n",
    "            en_twits.append(twitter[i].get('extended_tweet'))\n",
    "twits_text = []\n",
    "for line in en_twits:\n",
    "    twits_text.append(line['full_text'])\n",
    "for i in twits_text:\n",
    "    p = re.compile(\"([^h+t+t+p+s][^0-9][^''][a-zA-Z-']+)\")\n",
    "    p.findall(str(twits_text).lower().replace('hzzmhz', '').replace('co', '').replace('/t', '').replace('?', '').replace('.', '').replace('!', '').replace(',', '').replace(':', '').replace('https', '').replace('\\\\n', '').replace('.co', ''))\n",
    "pprint.pprint(Counter(words_twitter).most_common(163))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–¥–ø–∏—Å—á–∏–∫–æ–≤ (—Ñ–æ–ª–ª–æ–≤–µ—Ä–æ–≤) —É –∞–≤—Ç–æ—Ä–æ–≤ —Ç–≤–∏—Ç–æ–≤ ,                                      –≤—ã–≤–æ–¥ —Ç–æ–ø-10 –≤ —Ñ–æ—Ä–º–∞—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å - –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–æ–≤–µ—Ä–æ–≤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2521403, 1491309, 1206759, 1137374, 625463, 625463, 392472, 383698, 374222, 318189]\n",
      "Filosof√≠a‚ôï 2521403\n",
      "FITNESS Magazine 1491309\n",
      "malaysiakini.com 1206759\n",
      "NYT Science 1137374\n",
      "Gram√°tica 625463\n",
      "Gram√°tica 625463\n",
      "TGRT Haber 392472\n",
      "The Sun Football ‚öΩ 383698\n",
      "Melbourne, Australia 374222\n",
      "Roznama Express 318189\n",
      "üíû ·É™≈≥‡Ωû…†…õ‡Ωû·É™∆°∆°…†ƒ±…õ üíû 311319\n"
     ]
    }
   ],
   "source": [
    "followers_numb = []  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–æ–ª–æ–≤–µ—Ä–æ–≤\n",
    "for line in not_deleted:\n",
    "        user_info = line['user']\n",
    "        followers_numb.append(user_info.get('followers_count'))\n",
    "      \n",
    "followers_sort = []  # —Ñ–æ–ª–æ–≤–µ—Ä—ã –ø–æ —É–±—ã–≤–∞–Ω–∏—é\n",
    "for i in range(len(followers_numb)):\n",
    "    followers_sort.append(max(followers_numb))\n",
    "    followers_numb.remove(max(followers_numb))\n",
    "print(followers_sort[0:10])\n",
    "# –ø—Ä–∏ —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–µ —É–¥–∞–ª–∏–ª–æ—Å—å followers_sort, —Å–æ–∑–¥–∞–¥–∏–º –∫–æ–ø–∏—é\n",
    "followers_numb2 = []  \n",
    "for line in not_deleted:\n",
    "        user_info = line['user']\n",
    "        followers_numb2.append(user_info.get('followers_count'))\n",
    "\n",
    "top = []  # —Ç–æ–ø —Ñ–æ–ª–æ–≤–µ—Ä–æ–≤\n",
    "for line in not_deleted:\n",
    "    user = line['user']\n",
    "    top.append(user.get('name'))\n",
    "all_together = dict(zip(followers_numb2, top))  # —Å–ª–æ–≤–∞—Ä—å —é–∑–µ—Ä:—Ñ–æ–ª–æ–≤–µ—Ä\n",
    "n = 0\n",
    "while n < 11:\n",
    "    a = followers_sort[n]\n",
    "    print(all_together[a], a)\n",
    "    n = n + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
